{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from glob import glob\n",
    "import dask.bag as db\n",
    "import bs4\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm.notebook import tqdm\n",
    "from functions.scrape import get_review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/urls.txt') as f:\n",
    "    urls = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning! Monitor the loop for Captchas\n",
    "\n",
    "# If you send more than 900 requests, you'll get the following error:\n",
    "# ConnectionError: HTTPSConnectionPool(host='www.yelp.com', port=443):\n",
    "# Max retries exceeded with url: /biz/planet-granite-san-francisco-2?start=260\n",
    "# (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fb9a31254e0>:\n",
    "# Failed to establish a new connection: [Errno -2] Name or service not known',)\n",
    "\n",
    "for url in tqdm(urls):\n",
    "\n",
    "    # Base Page\n",
    "    ua = UserAgent() # Generate random user to make it seem like the task is distributed across multiple computers\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "    response  = requests.get(url, headers = user_agent)\n",
    "    soup = bs4.BeautifulSoup(response.text)    \n",
    "    \n",
    "    # Print information\n",
    "    page_text = soup.find('div',{\"class\":\"page-of-pages arrange_unit arrange_unit--fill\"}).get_text().strip() # Get 'Page <current> of <total>'\n",
    "    num_pages = int(re.search('(?<=of) [0-9]*', page_text).group(0).strip()) # Get total number of pages\n",
    "    num_reviews = soup.find('span',{\"class\":\"review-count rating-qualifier\"}).get_text().strip() # Get total number of reviews\n",
    "    business_id = soup.find(\"div\", {\"class\": \"lightbox-map hidden\"}).get(\"data-business-id\")\n",
    "#     print(*[url, business_id, 'Index: '+str(url_index), str(num_pages) + ' pages', num_reviews], sep=\"\\n\") # Quality Control\n",
    "    \n",
    "    # Generate list of webpages\n",
    "    start, end = 20, num_pages*20\n",
    "    pages = [url]\n",
    "    while start < end:\n",
    "        pages.append(url + '?start=' + str(start))\n",
    "        start += 20\n",
    "    \n",
    "    data_json = [] # Collect results\n",
    "    \n",
    "    for page in tqdm(pages):\n",
    "\n",
    "        time.sleep(10+10*random.random()) # So Yelp doesn't block IP\n",
    "\n",
    "        ua = UserAgent() # Generate random user to make it seem like the task is distributed across multiple computers\n",
    "        user_agent = {'User-agent': ua.random}\n",
    "        response  = requests.get(page, headers = user_agent)\n",
    "        soup = bs4.BeautifulSoup(response.text)\n",
    "\n",
    "        review = soup.findAll('div',{\"class\":\"review--with-sidebar\"})\n",
    "        data_list = get_review_data(review, business_id)\n",
    "        \n",
    "        for data in data_list:\n",
    "            data_json.append(data) # Collect data\n",
    "\n",
    "        # print(\"Collected page: \", pages.index(page)+1)\n",
    "\n",
    "    # Print final results\n",
    "    # print(*[data_json, len(data_json)], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Save JSON\n",
    "if save:\n",
    "    filename = re.match('https://www.yelp.com/biz/(?P<filename>.*)', urls[0])['filename'].replace('-', '_')\n",
    "    with open('data/reviews/{}.json'.format(filename), 'w') as f:\n",
    "        for review in data_json:\n",
    "            json.dump(review, f)\n",
    "            f.write('\\n')\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "review_full_bag = db.read_text(\"data/reviews/diablo_rock_gym_concord_3.json\").map(json.loads) # Loads the json file as a dask bag\n",
    "review_tuple = review_full_bag.take(10000) # Takes the first 10000 entries of the dask bag and stores as a tuple\n",
    "len(review_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aesthetic_climbing_gym_lake_forest',\n",
       " 'berkeley_ironworks_climbing_and_fitness_club_berkeley',\n",
       " 'blue_granite_climbing_gym_south_lake_tahoe',\n",
       " 'boulderdash_indoor_rock_climbing_thousand_oaks',\n",
       " 'boulderdash_sfv_chatsworth_6']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the files in the folder\n",
    "filenames_list = glob(\"data/reviews/*.json\") # Grab a list of filenames\n",
    "files = list(map(lambda x: x.replace(\"reviews/\", \"\").replace(\".json\",\"\").replace(\"data/\", \"\"), sorted(filenames_list)))\n",
    "files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e0c9d878f649539b894f7774672123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8317\n"
     ]
    }
   ],
   "source": [
    "# Concat\n",
    "climbing_reviews = []\n",
    "for file in tqdm(files):\n",
    "    review_full_bag = db.read_text(\"data/reviews/{}.json\".format(file)).map(json.loads) # Loads the json file as a dask bag\n",
    "    review_tuple = review_full_bag.take(10000) # Takes the first 10000 entries of the dask bag and stores as a tuple\n",
    "    climbing_reviews.extend(review_tuple)\n",
    "\n",
    "print(len(climbing_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "if save:\n",
    "    filename = \"reviews-copy\"\n",
    "    with open(f'data/{filename}.json', 'w') as f:\n",
    "        for review in climbing_reviews:\n",
    "            json.dump(review, f)\n",
    "            f.write('\\n')\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
